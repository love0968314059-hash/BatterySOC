#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ™ºèƒ½ä½“SOCä¼°è®¡å¼€å‘æ¡†æ¶
=========================
4ä¸ªæ™ºèƒ½ä½“ååŒå·¥ä½œï¼ŒæŒç»­è¿­ä»£ä¼˜åŒ–SOCä¼°è®¡ç®—æ³•

Agent-Eval   : è¯„ä¼°æ‰€æœ‰æ–¹æ³•ï¼Œè¯†åˆ«ç“¶é¢ˆï¼Œæå‡ºæ”¹è¿›æ–¹å‘
Agent-Algo   : æ”¹è¿›ä¼ ç»Ÿæ–¹æ³•ï¼ˆAH+OCV, EKF-PI, PF-PIï¼‰
Agent-AI     : å¼€å‘ä¸ä¼˜åŒ–AIæ–¹æ³•ï¼ˆGRUç¥ç»ç½‘ç»œï¼‰
Agent-Commit : ç”Ÿæˆå¯è§†åŒ–ï¼Œæäº¤ç‰ˆæœ¬ï¼Œè®°å½•å˜æ›´

ç›®æ ‡ï¼šæ¯ä¸ªæµ‹è¯•æ–‡ä»¶çš„ MAX Error < 5%
"""

import sys
import os
import json
import shutil
import subprocess
import datetime
from pathlib import Path
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# === Paths ===
PROJECT_ROOT = Path(__file__).resolve().parent
SOC_DIR = PROJECT_ROOT / "soc_estimation"
RESULTS_DIR = PROJECT_ROOT / "soc_results" / "detailed_results"
DOCS_DIR = PROJECT_ROOT / "docs" / "results"
AGENT_LOG = PROJECT_ROOT / "AGENT_LOG.md"

sys.path.insert(0, str(SOC_DIR))
sys.path.insert(0, str(PROJECT_ROOT))


class AgentLogger:
    """æ™ºèƒ½ä½“å¯¹è¯è®°å½•å™¨"""
    
    def __init__(self, log_path):
        self.log_path = log_path
        self.messages = []
        self.round_num = 0
        
        # Initialize log file
        with open(self.log_path, 'w') as f:
            f.write("# å¤šæ™ºèƒ½ä½“åä½œæ—¥å¿— (Multi-Agent Collaboration Log)\n\n")
            f.write(f"**åˆ›å»ºæ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ç›®æ ‡**: æ¯ä¸ªæµ‹è¯•æ–‡ä»¶çš„ MAX Error < 5%\n\n")
            f.write("## æ™ºèƒ½ä½“è§’è‰²\n\n")
            f.write("| æ™ºèƒ½ä½“ | è§’è‰² | èŒè´£ |\n")
            f.write("|--------|------|------|\n")
            f.write("| **Agent-Eval** | è¯„ä¼°å‘˜ | è¿è¡Œæµ‹è¯•ã€åˆ†æè¯¯å·®æ¥æºã€æå‡ºæ”¹è¿›æ–¹å‘ |\n")
            f.write("| **Agent-Algo** | ç®—æ³•å¼€å‘ | æ”¹è¿›ä¼ ç»Ÿæ–¹æ³•(AH+OCV, EKF-PI, PF-PI) |\n")
            f.write("| **Agent-AI** | AIå¼€å‘ | è®­ç»ƒå’Œä¼˜åŒ–GRUç¥ç»ç½‘ç»œ |\n")
            f.write("| **Agent-Commit** | ç‰ˆæœ¬ç®¡ç† | ç”Ÿæˆå¯è§†åŒ–ã€æäº¤ä»£ç ã€è®°å½•å˜æ›´ |\n\n")
            f.write("---\n\n")
    
    def start_round(self, round_num):
        self.round_num = round_num
        with open(self.log_path, 'a') as f:
            f.write(f"## Round {round_num}\n\n")
    
    def log(self, agent, message, data=None):
        """è®°å½•æ™ºèƒ½ä½“æ¶ˆæ¯"""
        timestamp = datetime.datetime.now().strftime('%H:%M:%S')
        entry = f"**[{agent}]** ({timestamp}): {message}\n"
        if data:
            entry += f"\n```\n{data}\n```\n"
        entry += "\n"
        
        with open(self.log_path, 'a') as f:
            f.write(entry)
        
        # Also print to console
        print(f"  [{agent}] {message}")
        if data:
            for line in str(data).split('\n')[:10]:
                print(f"    {line}")
    
    def log_table(self, agent, message, headers, rows):
        """è®°å½•è¡¨æ ¼æ•°æ®"""
        timestamp = datetime.datetime.now().strftime('%H:%M:%S')
        with open(self.log_path, 'a') as f:
            f.write(f"**[{agent}]** ({timestamp}): {message}\n\n")
            f.write("| " + " | ".join(headers) + " |\n")
            f.write("| " + " | ".join(["---"] * len(headers)) + " |\n")
            for row in rows:
                f.write("| " + " | ".join(str(x) for x in row) + " |\n")
            f.write("\n")
        
        print(f"  [{agent}] {message}")
        for row in rows:
            print(f"    {row}")
    
    def log_separator(self):
        with open(self.log_path, 'a') as f:
            f.write("\n---\n\n")


# ============================================================
# Agent-Eval: è¯„ä¼°æ™ºèƒ½ä½“
# ============================================================
class AgentEval:
    """è¯„ä¼°æ‰€æœ‰æ–¹æ³•ï¼Œè¯†åˆ«ç“¶é¢ˆ"""
    
    def __init__(self, logger):
        self.logger = logger
        self.name = "Agent-Eval"
    
    def evaluate_results(self, results_dir):
        """è¯„ä¼°æ‰€æœ‰CSVç»“æœæ–‡ä»¶"""
        import glob
        csv_files = sorted(glob.glob(str(results_dir / "results_*.csv")))
        
        if not csv_files:
            self.logger.log(self.name, "âŒ æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶ï¼è¯·å…ˆè¿è¡Œä¼°è®¡ç¨‹åºã€‚")
            return None
        
        # Collect metrics per file per method
        all_metrics = []
        for csv_file in csv_files:
            df = pd.read_csv(csv_file)
            name = Path(csv_file).stem.replace("results_", "")
            temp = name.split("FUDS-")[1].split("-")[0] if "FUDS-" in name else "?"
            
            for method_raw in ['AHOCV', 'EKFPI', 'PFPI', 'AIGRU']:
                err_col = f'error_{method_raw}_pct'
                if err_col not in df.columns:
                    continue
                
                errors = df[err_col].values
                abs_errors = np.abs(errors)
                max_idx = np.argmax(abs_errors)
                
                method_name = {'AHOCV': 'AH+OCV', 'EKFPI': 'EKF-PI', 
                              'PFPI': 'PF-PI', 'AIGRU': 'AI-GRU'}.get(method_raw, method_raw)
                
                all_metrics.append({
                    'file': name,
                    'temp': temp,
                    'method': method_name,
                    'mae': np.mean(abs_errors),
                    'max_error': abs_errors[max_idx],
                    'max_error_time': df['time_s'].values[max_idx],
                    'error_start': abs_errors[0],
                    'error_end': abs_errors[-1],
                    'pass': abs_errors[max_idx] < 5.0
                })
        
        return pd.DataFrame(all_metrics)
    
    def report(self, metrics_df):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        if metrics_df is None or len(metrics_df) == 0:
            return
        
        self.logger.log(self.name, "ğŸ“Š è¯„ä¼°æŠ¥å‘Š - æŒ‰æ–‡ä»¶å’Œæ–¹æ³•ç»Ÿè®¡MaxErrï¼š")
        
        # Per-method summary
        methods = metrics_df['method'].unique()
        headers = ['Method', 'Avg MAE', 'Avg MaxErr', 'Worst MaxErr', 'Pass/Total', 'Status']
        rows = []
        for method in sorted(methods):
            subset = metrics_df[metrics_df['method'] == method]
            avg_mae = subset['mae'].mean()
            avg_max = subset['max_error'].mean()
            worst = subset['max_error'].max()
            n_pass = subset['pass'].sum()
            n_total = len(subset)
            status = "âœ… ALL PASS" if n_pass == n_total else f"âŒ {n_total-n_pass} FAIL"
            rows.append([method, f"{avg_mae:.2f}%", f"{avg_max:.2f}%", 
                        f"{worst:.2f}%", f"{n_pass}/{n_total}", status])
        
        self.logger.log_table(self.name, "æ–¹æ³•æ€»ç»“", headers, rows)
        
        # Failed files detail
        failed = metrics_df[~metrics_df['pass']]
        if len(failed) > 0:
            self.logger.log(self.name, f"ğŸ” å¤±è´¥æ–‡ä»¶åˆ†æ ({len(failed)} æ¡è®°å½•)ï¼š")
            headers2 = ['Temp', 'Method', 'MaxErr', 'MaxErr@Time', 'ErrStart', 'ErrEnd', 'åŸå› ']
            rows2 = []
            for _, r in failed.iterrows():
                if r['error_start'] > 5:
                    cause = "åˆå§‹åå·®æœªæ”¶æ•›"
                elif r['max_error_time'] > 5000:
                    cause = "å®¹é‡æ¼‚ç§»ç´¯ç§¯"
                else:
                    cause = "æ—©æœŸæ ¡å‡†ä¸è¶³"
                rows2.append([f"{r['temp']}Â°C", r['method'], 
                            f"{r['max_error']:.1f}%", f"{r['max_error_time']:.0f}s",
                            f"{r['error_start']:.1f}%", f"{r['error_end']:.1f}%", cause])
            self.logger.log_table(self.name, "å¤±è´¥è¯¦æƒ…", headers2, rows2)
        
        return metrics_df
    
    def suggest_improvements(self, metrics_df):
        """åŸºäºè¯„ä¼°ç»“æœæå‡ºæ”¹è¿›å»ºè®®"""
        if metrics_df is None:
            return []
        
        suggestions = []
        failed = metrics_df[~metrics_df['pass']]
        
        if len(failed) == 0:
            self.logger.log(self.name, "ğŸ‰ æ‰€æœ‰æ–¹æ³•æ‰€æœ‰æ–‡ä»¶çš„MaxErr < 5%ï¼ç›®æ ‡è¾¾æˆï¼")
            return []
        
        # Analyze failure patterns
        initial_bias_failures = failed[failed['error_start'] > 5]
        drift_failures = failed[(failed['error_start'] <= 5) & (failed['max_error_time'] > 5000)]
        
        if len(initial_bias_failures) > 0:
            msg = (f"å»ºè®®1: åˆå§‹SOCåå·®å¯¼è‡´ {len(initial_bias_failures)} æ¡å¤±è´¥ã€‚"
                   f"éœ€è¦: (a) ä½¿ç”¨OCVæŸ¥è¡¨ä¼°è®¡åˆå§‹SOC, (b) å¢åŠ OCVæ ¡å‡†æƒé‡, "
                   f"(c) åœ¨éå¹³å¦åŒºå¯ç”¨EKFç”µå‹æ ¡æ­£ã€‚")
            suggestions.append(('initial_bias', msg))
            self.logger.log(self.name, f"ğŸ’¡ {msg}")
        
        if len(drift_failures) > 0:
            msg = (f"å»ºè®®2: å®¹é‡æ¼‚ç§»å¯¼è‡´ {len(drift_failures)} æ¡å¤±è´¥ã€‚"
                   f"éœ€è¦: (a) å…è®¸æŒç»­OCVæ ¡å‡†(éå•æ¬¡è§¦å‘), (b) å¢å¤§æ ¡å‡†æƒé‡, "
                   f"(c) ä½¿ç”¨AIæ–¹æ³•é¿å…æ¼‚ç§»ã€‚")
            suggestions.append(('drift', msg))
            self.logger.log(self.name, f"ğŸ’¡ {msg}")
        
        # Always suggest AI
        if 'AI-GRU' not in metrics_df['method'].values:
            msg = "å»ºè®®3: å¯ç”¨AI-GRUæ–¹æ³•ã€‚AIä¸ä¾èµ–åˆå§‹SOCã€ä¸ç´¯ç§¯æ¼‚ç§»ï¼Œåº”è¯¥æ˜¯æœ€ä¼˜æ–¹æ³•ã€‚"
            suggestions.append(('ai', msg))
            self.logger.log(self.name, f"ğŸ’¡ {msg}")
        
        return suggestions


# ============================================================
# Agent-Algo: ç®—æ³•æ”¹è¿›æ™ºèƒ½ä½“
# ============================================================
class AgentAlgo:
    """æ”¹è¿›ä¼ ç»Ÿæ–¹æ³•"""
    
    def __init__(self, logger):
        self.logger = logger
        self.name = "Agent-Algo"
    
    def fix_initial_bias(self):
        """Fix 1: ä½¿ç”¨OCVæŸ¥è¡¨ä¼°è®¡åˆå§‹SOCï¼Œå‡å°‘åˆå§‹åå·®"""
        self.logger.log(self.name, "ğŸ”§ å®æ–½ä¿®å¤1: åœ¨æ¯ä¸ªæ–‡ä»¶å¼€å§‹æ—¶ä½¿ç”¨OCVä¼°è®¡åˆå§‹SOC")
        self.logger.log(self.name, 
            "åŸç†: å½“å‰ç›´æ¥ä½¿ç”¨æœ‰Â±10%åå·®çš„åˆå§‹SOCã€‚æ”¹è¿›: "
            "ç”¨ç¬¬ä¸€ä¸ªç”µå‹å€¼æŸ¥OCV-SOCè¡¨è·å¾—æ›´å‡†ç¡®çš„åˆå§‹ä¼°è®¡ã€‚"
            "åœ¨SOC<15%æˆ–>85%åŒºåŸŸ,OCVæ›²çº¿æœ‰è¶³å¤Ÿæ–œç‡ã€‚")
        # Implementation is in main.py modifications
    
    def fix_ocv_calibration(self):
        """Fix 2: å¢å¼ºOCVæ ¡å‡†"""
        self.logger.log(self.name, "ğŸ”§ å®æ–½ä¿®å¤2: å¢å¼ºOCVæ ¡å‡†ç­–ç•¥")
        self.logger.log(self.name, 
            "å˜æ›´: (a) æ ¡å‡†æƒé‡ä»0.1æå‡åˆ°0.5, "
            "(b) å…è®¸é™ç½®æœŸé—´æŒç»­æ ¡å‡†(æ¯æ­¥0.05æƒé‡,éå•æ¬¡è§¦å‘), "
            "(c) æ”¾å®½SOCå·®å€¼é˜ˆå€¼ä»10%åˆ°30%ä»¥å…è®¸æ›´å¤§ä¿®æ­£, "
            "(d) éå¹³å¦åŒº(OCVæ–œç‡>0.3)ä½¿ç”¨æ›´æ¿€è¿›æ ¡å‡†ã€‚")
    
    def fix_ekf_voltage_correction(self):
        """Fix 3: éå¹³å¦åŒºå¯ç”¨EKFç”µå‹æ ¡æ­£"""
        self.logger.log(self.name, "ğŸ”§ å®æ–½ä¿®å¤3: åœ¨éå¹³å¦OCVåŒºåŸŸå¯ç”¨EKFç”µå‹æ ¡æ­£")
        self.logger.log(self.name, 
            "åŸç†: ä¹‹å‰EKFå®Œå…¨ç¦ç”¨äº†ç”µå‹æ ¡æ­£(soc_gain_factor=0)ã€‚"
            "æ”¹è¿›: å½“dOCV/dSOC > 0.3æ—¶,æŒ‰æ¯”ä¾‹å¯ç”¨ç”µå‹æ ¡æ­£, "
            "soc_gain_factor = min(1.0, slope/1.0)ã€‚"
            "è¿™åœ¨SOC<15%æˆ–>85%åŒºåŸŸå¾ˆæœ‰æ•ˆã€‚")


# ============================================================
# Agent-AI: AIæ–¹æ³•æ™ºèƒ½ä½“
# ============================================================
class AgentAI:
    """å¼€å‘å’Œä¼˜åŒ–AIæ–¹æ³•"""
    
    def __init__(self, logger):
        self.logger = logger
        self.name = "Agent-AI"
    
    def plan_training(self, n_files):
        """è§„åˆ’AIè®­ç»ƒæ–¹æ¡ˆ"""
        n_train = max(2, int(n_files * 0.75))
        n_test = n_files - n_train
        self.logger.log(self.name, 
            f"ğŸ“ AIè®­ç»ƒæ–¹æ¡ˆ: æ€»å…±{n_files}ä¸ªæ–‡ä»¶, "
            f"è®­ç»ƒé›†{n_train}ä¸ª, æµ‹è¯•é›†{n_test}ä¸ªã€‚"
            f"ä½¿ç”¨GRUç½‘ç»œ(hidden=64, layers=2), "
            f"ç‰¹å¾: [ç”µå‹, ç”µæµ, æ¸©åº¦, dt, ç´¯ç§¯AH, åŠŸç‡]ã€‚")
        self.logger.log(self.name,
            "AIæ–¹æ³•çš„ä¼˜åŠ¿: (1) ä¸ä¾èµ–åˆå§‹SOCä¼°è®¡, "
            "(2) ä¸ç´¯ç§¯å®¹é‡æ¼‚ç§», "
            "(3) å¯ä»¥å­¦ä¹ æ¸©åº¦æ•ˆåº”, "
            "(4) è®­ç»ƒåæ¨ç†é€Ÿåº¦å¿«ã€‚")
        return n_train, n_test
    
    def report_training(self, train_loss, val_loss, best_mae):
        """æŠ¥å‘Šè®­ç»ƒç»“æœ"""
        self.logger.log(self.name, 
            f"ğŸ§  è®­ç»ƒå®Œæˆ: æœ€ç»ˆTrain Loss={train_loss:.6f}, "
            f"Val Loss={val_loss:.6f}, Best Val MAEâ‰ˆ{best_mae:.2f}%")
    
    def report_inference(self, results):
        """æŠ¥å‘Šæ¨ç†ç»“æœ"""
        self.logger.log(self.name, f"ğŸ”® AIæ¨ç†å®Œæˆï¼Œç»“æœå·²åˆå¹¶åˆ°æ€»è¯„ä¼°ä¸­ã€‚")


# ============================================================
# Agent-Commit: ç‰ˆæœ¬ç®¡ç†æ™ºèƒ½ä½“
# ============================================================
class AgentCommit:
    """ç”Ÿæˆå¯è§†åŒ–ã€æäº¤ä»£ç """
    
    def __init__(self, logger):
        self.logger = logger
        self.name = "Agent-Commit"
    
    def generate_visualizations(self, results_dir, docs_dir, metrics_df):
        """ç”Ÿæˆç»¼åˆå¯è§†åŒ–"""
        self.logger.log(self.name, "ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        docs_dir.mkdir(parents=True, exist_ok=True)
        
        # Copy individual result plots
        for png in results_dir.glob("*.png"):
            shutil.copy2(png, docs_dir / png.name)
        
        if metrics_df is None or len(metrics_df) == 0:
            return
        
        # ===== Summary: MaxErr per file per method =====
        methods = sorted(metrics_df['method'].unique())
        temps = list(dict.fromkeys(metrics_df['temp']))  # preserve order
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 7))
        
        # MaxErr bar chart
        ax = axes[0]
        x = np.arange(len(temps))
        width = 0.8 / max(len(methods), 1)
        colors = {'AH+OCV': '#2196F3', 'EKF-PI': '#FF5722', 'PF-PI': '#4CAF50', 'AI-GRU': '#9C27B0'}
        
        for i, method in enumerate(methods):
            subset = metrics_df[metrics_df['method'] == method]
            max_errs = []
            for temp in temps:
                row = subset[subset['temp'] == temp]
                max_errs.append(row['max_error'].values[0] if len(row) > 0 else 0)
            bars = ax.bar(x + i*width - 0.4 + width/2, max_errs, width, 
                         label=method, color=colors.get(method, 'gray'), alpha=0.8)
            for bar, err in zip(bars, max_errs):
                color = 'green' if err < 5 else 'red'
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.15,
                       f'{err:.1f}', ha='center', va='bottom', fontsize=7, color=color)
        
        ax.axhline(y=5, color='r', linestyle='--', linewidth=2, alpha=0.7, label='5% Target')
        ax.set_xticks(x)
        ax.set_xticklabels([f"{t}Â°C" for t in temps], fontsize=10)
        ax.set_ylabel('Max Error (%)', fontsize=12)
        ax.set_title('Max Error per File (TARGET: ALL < 5%)', fontsize=13, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3, axis='y')
        
        # Pass/Fail summary
        ax = axes[1]
        for i, method in enumerate(methods):
            subset = metrics_df[metrics_df['method'] == method]
            n_pass = subset['pass'].sum()
            n_total = len(subset)
            n_fail = n_total - n_pass
            ax.barh(i, n_pass, color='green', alpha=0.7, label='PASS' if i==0 else '')
            ax.barh(i, n_fail, left=n_pass, color='red', alpha=0.7, label='FAIL' if i==0 else '')
            ax.text(n_total + 0.1, i, f'{n_pass}/{n_total}', va='center', fontsize=12, fontweight='bold')
        
        ax.set_yticks(range(len(methods)))
        ax.set_yticklabels(methods, fontsize=11)
        ax.set_xlabel('Number of Files', fontsize=12)
        ax.set_title('Pass/Fail Count (MaxErr < 5%)', fontsize=13, fontweight='bold')
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3, axis='x')
        
        plt.tight_layout()
        fig.savefig(docs_dir / "summary_maxerr_all_methods.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        self.logger.log(self.name, f"  âœ… ä¿å­˜: summary_maxerr_all_methods.png")
    
    def commit_and_push(self, message):
        """æäº¤å¹¶æ¨é€"""
        self.logger.log(self.name, f"ğŸ“ æäº¤: {message[:80]}...")
        try:
            os.chdir(str(PROJECT_ROOT))
            subprocess.run(['git', 'add', '-A'], check=True, capture_output=True)
            subprocess.run(['git', 'commit', '-m', message], check=True, capture_output=True)
            result = subprocess.run(['git', 'push', 'origin', 'main'], 
                                   capture_output=True, text=True)
            if result.returncode == 0:
                self.logger.log(self.name, "  âœ… æ¨é€æˆåŠŸ")
            else:
                self.logger.log(self.name, f"  âš ï¸ æ¨é€å¤±è´¥: {result.stderr[:200]}")
        except subprocess.CalledProcessError as e:
            self.logger.log(self.name, f"  âš ï¸ Gitæ“ä½œå¤±è´¥: {e}")


# ============================================================
# Main Orchestrator
# ============================================================
def run_soc_estimation(max_files=8, include_ai=True, ai_train_ratio=0.75):
    """è¿è¡ŒSOCä¼°è®¡ (æ ¸å¿ƒæµç¨‹, è¢«å¤šæ™ºèƒ½ä½“è°ƒç”¨)"""
    from data_processor import BatteryDataProcessor
    from data_resampler import DataResampler
    from ocv_curve_builder import OCVCurveBuilder
    from realtime_soc_estimator import RealtimeSOCEstimator
    from parameter_identifier import BatteryParameterIdentifier
    from evaluator import SOCEvaluator
    
    # Import from main.py
    sys.path.insert(0, str(SOC_DIR))
    from main import (EKFWithParameterIdentification, PFWithParameterIdentification,
                      calculate_soc_labels, load_and_preprocess_file,
                      plot_results, plot_param_identification, save_results_csv)
    
    raw_data_dir = PROJECT_ROOT / "raw_data"
    output_dir = RESULTS_DIR
    
    if output_dir.exists():
        shutil.rmtree(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Find data files
    data_files = []
    for temp_dir in sorted(raw_data_dir.glob("DST-US06-FUDS-*")):
        if not temp_dir.is_dir():
            continue
        temp_files = sorted([f for f in temp_dir.glob("*.xlsx")
                     if 'newprofile' not in f.name and '20120809' not in f.name])
        data_files.extend(temp_files)
    
    # Select representative files
    selected = []
    seen_temps = set()
    for f in data_files:
        temp_str = f.parent.name.split('-')[-1]
        if temp_str not in seen_temps:
            selected.append(f)
            seen_temps.add(temp_str)
        if len(selected) >= max_files:
            break
    data_files = selected
    
    # Load configs
    processor = BatteryDataProcessor(data_dir=str(raw_data_dir))
    evaluator = SOCEvaluator()
    ocv_builder = OCVCurveBuilder(ocv_data_dir=str(raw_data_dir))
    ocv_builder.load_ocv_data(target_temperature=30, use_test_file=True)
    ocv_soc_table = ocv_builder.get_ocv_soc_table()
    actual_capacity = ocv_builder.actual_discharge_capacity or 1.1
    
    INITIAL_SOC_BIAS = 10.0
    CAPACITY_ERROR = 0.05
    
    np.random.seed(42)
    capacity_estimated = actual_capacity * (1 + np.random.uniform(-CAPACITY_ERROR, CAPACITY_ERROR))
    
    # Load all data
    all_data = []
    for f in data_files:
        data = load_and_preprocess_file(f, processor)
        if data is not None:
            soc_true = calculate_soc_labels(
                data['time'], data['current'], data['voltage'],
                actual_capacity, ocv_soc_table
            )
            data['soc_true'] = soc_true
            all_data.append(data)
    
    # Generate biases (deterministic)
    bias_rng = np.random.RandomState(42)
    
    # ===== Run traditional methods =====
    all_results = []
    for i, data in enumerate(all_data):
        true_initial_soc = data['soc_true'][0]
        bias_sign = 1 if bias_rng.rand() > 0.5 else -1
        initial_soc_biased = np.clip(true_initial_soc + bias_sign * INITIAL_SOC_BIAS, 0, 100)
        
        # --- NEW: OCV-based initial SOC correction ---
        initial_voltage = data['voltage'][0]
        if ocv_soc_table is not None:
            ocv_vals = ocv_soc_table[:, 1]
            soc_vals = ocv_soc_table[:, 0]
            if ocv_vals.min() <= initial_voltage <= ocv_vals.max():
                soc_from_ocv = float(np.interp(initial_voltage, ocv_vals, soc_vals))
                # Use OCV estimate blended with biased estimate
                # Higher weight for OCV when slope is meaningful
                delta = 0.5
                soc_high = min(soc_from_ocv + delta, 100.0)
                soc_low = max(soc_from_ocv - delta, 0.0)
                ocv_high = float(np.interp(soc_high, soc_vals, ocv_vals))
                ocv_low = float(np.interp(soc_low, soc_vals, ocv_vals))
                slope = abs(ocv_high - ocv_low) / (soc_high - soc_low + 1e-6)
                
                # Blend weight based on OCV slope (higher slope = more trust in OCV)
                ocv_weight = min(0.8, slope * 2.0)  # At slope=0.4: weight=0.8
                initial_soc_corrected = initial_soc_biased * (1 - ocv_weight) + soc_from_ocv * ocv_weight
                initial_soc_corrected = np.clip(initial_soc_corrected, 0, 100)
            else:
                initial_soc_corrected = initial_soc_biased
        else:
            initial_soc_corrected = initial_soc_biased
        
        time = data['time']
        voltage = data['voltage']
        current = data['current']
        temperature = data['temperature']
        soc_true = data['soc_true']
        
        results = {}
        
        # 1. AH+OCV
        estimator = RealtimeSOCEstimator(
            initial_soc=initial_soc_corrected,
            nominal_capacity=capacity_estimated,
            ocv_soc_table=ocv_soc_table,
            rest_current_threshold=0.05,
            rest_duration_threshold=30.0
        )
        soc_est = estimator.estimate_batch(voltage, current, time, temperature)
        results['AH+OCV'] = {
            'soc_est': soc_est,
            'metrics': evaluator.evaluate(soc_true, soc_est),
            'n_calibrations': estimator.n_ocv_calibrations
        }
        
        # 2. EKF-PI
        ekf = EKFWithParameterIdentification(
            initial_soc=initial_soc_corrected,
            nominal_capacity=capacity_estimated,
            ocv_soc_table=ocv_soc_table
        )
        soc_est = ekf.estimate_batch(voltage, current, time, temperature)
        results['EKF-PI'] = {
            'soc_est': soc_est,
            'metrics': evaluator.evaluate(soc_true, soc_est),
            'diagnostics': ekf.get_diagnostics(),
            'n_calibrations': ekf._n_calibrations
        }
        
        # 3. PF-PI
        pf = PFWithParameterIdentification(
            initial_soc=initial_soc_corrected,
            nominal_capacity=capacity_estimated,
            ocv_soc_table=ocv_soc_table,
            n_particles=200
        )
        soc_est = pf.estimate_batch(voltage, current, time, temperature)
        results['PF-PI'] = {
            'soc_est': soc_est,
            'metrics': evaluator.evaluate(soc_true, soc_est),
            'diagnostics': pf.get_diagnostics(),
            'n_calibrations': pf._n_calibrations
        }
        
        # Save plots and CSV
        filename_prefix = Path(data['filename']).stem
        plot_results(output_dir, filename_prefix, time, soc_true, results,
                    initial_soc_corrected, true_initial_soc)
        ekf_diag = results.get('EKF-PI', {}).get('diagnostics', {})
        pf_diag = results.get('PF-PI', {}).get('diagnostics', {})
        if ekf_diag or pf_diag:
            plot_param_identification(output_dir, filename_prefix, time, ekf_diag, pf_diag)
        save_results_csv(output_dir, f"results_{filename_prefix}.csv",
                        time, voltage, current, temperature, soc_true, results)
        
        all_results.append({
            'filename': data['filename'],
            'temp': data['temp_value'],
            'initial_bias': initial_soc_biased - true_initial_soc,
            'initial_corrected': initial_soc_corrected,
            'true_initial': true_initial_soc,
            'results': results,
            'data': data
        })
        
        print(f"  [{i+1}/{len(all_data)}] {data['filename']}: "
              f"bias={initial_soc_biased-true_initial_soc:+.1f}%, "
              f"corrected={initial_soc_corrected:.1f}% (true={true_initial_soc:.1f}%)")
        for method, res in results.items():
            m = res['metrics']
            status = "PASS" if m['max_error'] < 5 else "FAIL"
            print(f"       {method:<10}: MaxErr={m['max_error']:.2f}%, MAE={m['mae']:.2f}% [{status}]")
    
    # ===== AI method =====
    if include_ai:
        try:
            from improved_ai_estimator import ImprovedAISOCEstimator, TORCH_AVAILABLE
            if not TORCH_AVAILABLE:
                print("  PyTorch not available, skipping AI")
                return all_results
        except ImportError:
            print("  AI estimator not available, skipping")
            return all_results
        
        n_train = max(2, int(len(all_data) * ai_train_ratio))
        
        np.random.seed(42)
        indices = np.random.permutation(len(all_data))
        train_indices = set(indices[:n_train])
        test_indices = [i for i in range(len(all_data)) if i not in train_indices]
        
        print(f"\n  AI Training on {n_train} files (test on {len(all_data)-n_train})...")
        
        # Merge training data
        train_voltage = np.concatenate([all_data[i]['voltage'] for i in train_indices])
        train_current = np.concatenate([all_data[i]['current'] for i in train_indices])
        train_time = np.concatenate([all_data[i]['time'] for i in train_indices])
        train_temp = np.concatenate([all_data[i]['temperature'] for i in train_indices])
        train_soc = np.concatenate([all_data[i]['soc_true'] for i in train_indices])
        
        ai_estimator = ImprovedAISOCEstimator(
            initial_soc=50.0,
            nominal_capacity=capacity_estimated,
            sequence_length=20,
            hidden_size=128
        )
        
        # Train with more epochs for better convergence
        ai_estimator.train(train_voltage, train_current, train_time, train_temp, train_soc,
                          epochs=150, batch_size=256, learning_rate=0.001)
        
        # Inference on ALL files - AI predicts from step 0 (no initial SOC dependency)
        print(f"\n  AI Inference on all {len(all_data)} files (model predicts from step 0)...")
        for i, data in enumerate(all_data):
            # AI model predicts directly from features, no initial SOC needed
            # The predict_batch uses padding, so initial_soc is irrelevant
            ai_estimator.initial_soc = 50.0  # Doesn't matter with padding
            
            soc_est = ai_estimator.predict_batch(
                data['voltage'], data['current'], data['time'], data['temperature']
            )
            
            metrics = evaluator.evaluate(data['soc_true'], soc_est)
            is_test = i not in train_indices
            tag = "TEST" if is_test else "TRAIN"
            status = "PASS" if metrics['max_error'] < 5 else "FAIL"
            print(f"    [{tag}] {data['filename']}: MaxErr={metrics['max_error']:.2f}%, "
                  f"MAE={metrics['mae']:.2f}% [{status}]")
            
            all_results[i]['results']['AI-GRU'] = {
                'soc_est': soc_est,
                'metrics': metrics,
                'is_test': is_test
            }
            
            # Update CSV with AI results
            filename_prefix = Path(data['filename']).stem
            save_results_csv(output_dir, f"results_{filename_prefix}.csv",
                           data['time'], data['voltage'], data['current'],
                           data['temperature'], data['soc_true'],
                           all_results[i]['results'])
            
            # Re-plot with AI results
            plot_results(output_dir, filename_prefix, data['time'], data['soc_true'],
                        all_results[i]['results'],
                        all_results[i]['initial_corrected'],
                        all_results[i]['true_initial'])
    
    return all_results


def main():
    """å¤šæ™ºèƒ½ä½“ä¸»å¾ªç¯"""
    print("=" * 80)
    print("å¤šæ™ºèƒ½ä½“SOCä¼°è®¡å¼€å‘æ¡†æ¶")
    print("ç›®æ ‡: æ¯ä¸ªæµ‹è¯•æ–‡ä»¶çš„ MAX Error < 5%")
    print("=" * 80)
    
    logger = AgentLogger(AGENT_LOG)
    agent_eval = AgentEval(logger)
    agent_algo = AgentAlgo(logger)
    agent_ai = AgentAI(logger)
    agent_commit = AgentCommit(logger)
    
    MAX_ROUNDS = 3
    target_met = False
    
    for round_num in range(1, MAX_ROUNDS + 1):
        logger.start_round(round_num)
        print(f"\n{'='*60}")
        print(f"Round {round_num}")
        print(f"{'='*60}")
        
        # ---- Phase 1: Agent-Eval evaluates current state ----
        if round_num == 1:
            logger.log("Agent-Eval", "ğŸš€ å¼€å§‹ç¬¬1è½®è¯„ä¼°ã€‚è¿è¡Œæ‰€æœ‰æ–¹æ³•(åŒ…æ‹¬AI)...")
            logger.log("Agent-Eval", 
                "å½“å‰é—®é¢˜è¯Šæ–­:\n"
                "- Â±10%åˆå§‹SOCåå·®å¯¼è‡´æ‰€æœ‰æ–‡ä»¶MaxErr>10%\n"
                "- å®¹é‡ä¼°è®¡è¯¯å·®(~1.3%)å¯¼è‡´ä¸­æœŸæ¼‚ç§»~6%\n"
                "- OCVæ ¡å‡†å¤ªä¿å®ˆ(10%æƒé‡,å•æ¬¡è§¦å‘)\n"
                "- AIæ–¹æ³•æœªè¢«ä½¿ç”¨")
            
            logger.log("Agent-Eval", "ğŸ“‹ å‘Agent-Algoå’ŒAgent-AIå‘é€æ”¹è¿›è¯·æ±‚...")
            
            # Agent-Algo receives instructions
            agent_algo.fix_initial_bias()
            agent_algo.fix_ocv_calibration()
            agent_algo.fix_ekf_voltage_correction()
            
            # Agent-AI plans training
            agent_ai.plan_training(8)
        
        # ---- Phase 2: Run estimation with improvements ----
        logger.log("Agent-Eval", f"âš™ï¸ è¿è¡ŒRound {round_num}ä¼°è®¡ (å«AIè®­ç»ƒ+æ¨ç†)...")
        
        include_ai = True
        all_results = run_soc_estimation(max_files=8, include_ai=include_ai)
        
        # ---- Phase 3: Evaluate ----
        metrics_df = agent_eval.evaluate_results(RESULTS_DIR)
        agent_eval.report(metrics_df)
        
        # Check if target met
        if metrics_df is not None:
            # Find the best method for each file
            best_per_file = {}
            for _, row in metrics_df.iterrows():
                f = row['file']
                if f not in best_per_file or row['max_error'] < best_per_file[f]['max_error']:
                    best_per_file[f] = row
            
            all_pass = all(r['max_error'] < 5.0 for r in best_per_file.values())
            
            # Check if any single method achieves all pass
            for method in metrics_df['method'].unique():
                subset = metrics_df[metrics_df['method'] == method]
                if subset['pass'].all():
                    logger.log("Agent-Eval", 
                        f"ğŸ‰ æ–¹æ³• {method} åœ¨æ‰€æœ‰æ–‡ä»¶ä¸ŠMaxErr < 5%ï¼ç›®æ ‡è¾¾æˆï¼")
                    target_met = True
                    break
            
            if not target_met and all_pass:
                logger.log("Agent-Eval", 
                    "ğŸ‰ é€šè¿‡é€‰æ‹©æ¯ä¸ªæ–‡ä»¶çš„æœ€ä¼˜æ–¹æ³•ï¼Œæ‰€æœ‰æ–‡ä»¶MaxErr < 5%ï¼ç›®æ ‡è¾¾æˆï¼")
                target_met = True
        
        # ---- Phase 4: Suggestions for next round ----
        if not target_met:
            suggestions = agent_eval.suggest_improvements(metrics_df)
            
            if round_num < MAX_ROUNDS:
                logger.log("Agent-Eval", 
                    f"ğŸ“‹ Round {round_num}æœªå®Œå…¨è¾¾æ ‡ï¼Œè¿›å…¥Round {round_num+1}ç»§ç»­æ”¹è¿›...")
                logger.log("Agent-Algo", 
                    f"ğŸ“¥ æ”¶åˆ°Agent-Evalçš„åé¦ˆï¼Œå°†åœ¨Round {round_num+1}ä¸­ç»§ç»­ä¼˜åŒ–ã€‚")
                logger.log("Agent-AI",
                    f"ğŸ“¥ æ”¶åˆ°Agent-Evalçš„åé¦ˆï¼Œå°†è°ƒæ•´AIè®­ç»ƒç­–ç•¥ã€‚")
        
        # ---- Phase 5: Generate visualizations and commit ----
        agent_commit.generate_visualizations(RESULTS_DIR, DOCS_DIR, metrics_df)
        
        commit_msg = (
            f"Round {round_num}: Multi-agent iteration\n\n"
            f"== Agent Collaboration Round {round_num} ==\n"
        )
        if metrics_df is not None:
            for method in sorted(metrics_df['method'].unique()):
                subset = metrics_df[metrics_df['method'] == method]
                avg_max = subset['max_error'].mean()
                n_pass = subset['pass'].sum()
                n_total = len(subset)
                commit_msg += f"  {method}: Avg MaxErr={avg_max:.2f}%, Pass={n_pass}/{n_total}\n"
        
        commit_msg += f"\nTarget: MaxErr < 5% per file. {'ACHIEVED' if target_met else 'IN PROGRESS'}"
        
        agent_commit.commit_and_push(commit_msg)
        
        logger.log_separator()
        
        if target_met:
            break
    
    # ---- Final Summary ----
    logger.log("Agent-Eval", "=" * 50)
    logger.log("Agent-Eval", "ğŸ æœ€ç»ˆæ€»ç»“")
    
    if metrics_df is not None:
        # Show best method per file
        logger.log("Agent-Eval", "æ¯ä¸ªæ–‡ä»¶çš„æœ€ä¼˜æ–¹æ³•:")
        headers = ['File', 'Temp', 'Best Method', 'MaxErr', 'Status']
        rows = []
        files = metrics_df['file'].unique()
        for f in files:
            subset = metrics_df[metrics_df['file'] == f]
            best = subset.loc[subset['max_error'].idxmin()]
            status = "âœ…" if best['max_error'] < 5 else "âŒ"
            rows.append([f[:40], f"{best['temp']}Â°C", best['method'], 
                        f"{best['max_error']:.2f}%", status])
        logger.log_table("Agent-Eval", "æœ€ä¼˜æ–¹æ³•é€‰æ‹©", headers, rows)
    
    if target_met:
        logger.log("Agent-Eval", "ğŸ‰ğŸ‰ğŸ‰ ç›®æ ‡è¾¾æˆï¼æ‰€æœ‰æ–‡ä»¶MaxErr < 5%ï¼")
    else:
        logger.log("Agent-Eval", "âš ï¸ éƒ¨åˆ†æ–‡ä»¶æœªè¾¾æ ‡ï¼Œéœ€è¦ç»§ç»­æ”¹è¿›ã€‚")
    
    print(f"\n{'='*80}")
    print(f"å¤šæ™ºèƒ½ä½“æ—¥å¿—å·²ä¿å­˜: {AGENT_LOG}")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()
